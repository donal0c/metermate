# Performance Test Execution Checklist

## Pre-Execution Checklist

### Environment Setup

- [ ] **Python version:** 3.11+
  ```bash
  python3 --version
  ```

- [ ] **Playwright installed:**
  ```bash
  python3 -m playwright install chromium
  python3 -m playwright --version
  ```

- [ ] **Dependencies installed:**
  ```bash
  cd /Users/donalocallaghan/workspace/vibes/steve/app
  pip install -r requirements.txt
  ```

- [ ] **Port 8600 available:**
  ```bash
  lsof -i :8600  # Should return nothing
  ```
  If port in use:
  ```bash
  lsof -i :8600 | grep LISTEN | awk '{print $2}' | xargs kill -9
  ```

### System Resources

- [ ] **Sufficient disk space:** > 5 GB free
  ```bash
  df -h .
  ```

- [ ] **Sufficient RAM:** > 4 GB available
  ```bash
  vm_stat | grep "Pages free"
  ```

- [ ] **No competing processes:** Close heavy applications (browser, IDE, etc.)

### Test Data

- [ ] **PDF bills available:**
  ```bash
  ls -la /Users/donalocallaghan/workspace/vibes/steve/Steve_bills/*.pdf
  ```
  Count: ___ PDFs found

- [ ] **HDF test files:** (Auto-generated by tests, no action needed)

- [ ] **Write permissions in temp directory:**
  ```bash
  touch /tmp/test_write_permission && rm /tmp/test_write_permission && echo "OK"
  ```

## Test Execution Phases

### Phase 1: Smoke Test (2 min)

**Purpose:** Verify basic setup works before running full suite.

```bash
cd /Users/donalocallaghan/workspace/vibes/steve

# Test 1: Can Streamlit start?
python3 -m pytest app/tests/test_e2e_performance.py::TestBrowserCompatibility::test_desktop_viewport_renders_correctly -v
```

**Expected result:** `PASSED` in ~30 seconds

- [ ] Test passed
- [ ] App started on port 8600
- [ ] No error messages

**If failed:** Check prerequisites above.

---

### Phase 2: Fast Critical Tests (10 min)

**Purpose:** Run most important performance tests.

```bash
# Test 2: Large file handling
python3 -m pytest app/tests/test_e2e_performance.py::TestLargeFileHandling -v

# Test 3: Concurrent operations
python3 -m pytest app/tests/test_e2e_performance.py::TestConcurrentOperations -v
```

**Expected results:**
- TestLargeFileHandling: 3 passed in ~5 min
- TestConcurrentOperations: 3 passed in ~3 min

Record results:
- [ ] `test_large_hdf_upload_completes_within_timeout` - Time: ___s (target < 60s)
- [ ] `test_large_hdf_memory_usage_stable` - Status: ___
- [ ] `test_large_pdf_bill_extraction_timeout_handling` - Status: ___
- [ ] `test_rapid_file_uploads_queued_properly` - Status: ___
- [ ] `test_upload_same_file_twice_handles_gracefully` - Status: ___
- [ ] `test_switch_tabs_rapidly_no_crash` - Status: ___

**If any failed:** Note error message and continue.

---

### Phase 3: Memory Leak Detection (10 min)

**Purpose:** Test for memory leaks over multiple uploads.

```bash
# Test 4: Sequential uploads
python3 -m pytest app/tests/test_e2e_performance.py::TestCacheAndState::test_sequential_uploads_memory_stable -v -s
```

**While running:** Monitor memory in separate terminal:
```bash
watch -n 1 'ps aux | grep streamlit | grep -v grep | awk "{print \$6/1024 \" MB\"}"'
```

Record memory usage:
- Upload 1: ___ MB
- Upload 2: ___ MB
- Upload 3: ___ MB
- Upload 4: ___ MB
- Upload 5: ___ MB
- Growth: ___% (should be < 10%)

- [ ] Memory growth < 10%
- [ ] Test passed

**If failed:** Memory leak detected. Review app code for unreleased resources.

---

### Phase 4: Timeout Scenarios (5 min)

**Purpose:** Test timeout handling and error messages.

```bash
# Test 5: Timeout scenarios
python3 -m pytest app/tests/test_e2e_performance.py::TestTimeoutScenarios -v
```

Record results:
- [ ] `test_slow_network_shows_progress_indicator` - Status: ___
- [ ] `test_very_large_file_timeout_error_message` - Status: ___

**Check:**
- [ ] Clear error messages shown on timeout
- [ ] App remains functional after timeout
- [ ] No hanging processes

---

### Phase 5: Browser Compatibility (5 min)

**Purpose:** Test responsive design across viewports.

```bash
# Test 6: Browser compatibility
python3 -m pytest app/tests/test_e2e_performance.py::TestBrowserCompatibility -v
```

Record results:
- [ ] Mobile (375x667): ___
- [ ] Tablet (768x1024): ___
- [ ] Desktop (1920x1080): ___

**Verify:**
- [ ] All viewports render correctly
- [ ] No layout overflow
- [ ] All features accessible

---

### Phase 6: Rapid User Actions (5 min)

**Purpose:** Test UI stability under aggressive interactions.

```bash
# Test 7: Rapid actions
python3 -m pytest app/tests/test_e2e_performance.py::TestRapidUserActions -v
```

Record results:
- [ ] `test_click_upload_button_rapidly_no_crash` - Status: ___
- [ ] `test_collapse_expand_sections_rapidly` - Status: ___

**Check:**
- [ ] No UI crashes
- [ ] No JavaScript errors
- [ ] App remains responsive

---

### Phase 7: Cache & State Management (5 min)

**Purpose:** Test cache invalidation and state transitions.

```bash
# Test 8: Cache and state
python3 -m pytest app/tests/test_e2e_performance.py::TestCacheAndState::test_upload_file_a_then_b_clears_a_data -v
python3 -m pytest app/tests/test_e2e_performance.py::TestCacheAndState::test_page_reload_clears_state -v
```

Record results:
- [ ] File A → File B transition clean
- [ ] Page reload clears state
- [ ] No data mixing

---

### Phase 8: Error Recovery (5 min)

**Purpose:** Test resilience and recovery from errors.

```bash
# Test 9: Error recovery
python3 -m pytest app/tests/test_e2e_performance.py::TestErrorRecovery -v
```

Record results:
- [ ] `test_invalid_file_shows_error_then_recovers` - Status: ___
- [ ] `test_network_interruption_during_upload` - Status: ___

**Verify:**
- [ ] App recovers from invalid files
- [ ] Clear error messages
- [ ] No permanent breakage

---

### Phase 9: Full Suite (Optional - 33 min)

**Purpose:** Run all tests for comprehensive validation.

```bash
# Test 10: Full suite
python3 -m pytest -m performance app/tests/test_e2e_performance.py -v --html=performance-report.html --self-contained-html
```

**Expected:** 22 tests passed in ~33 minutes

- [ ] All tests passed
- [ ] HTML report generated
- [ ] No critical failures

## Post-Execution Checklist

### Review Results

- [ ] **Total tests run:** ___ / 22
- [ ] **Tests passed:** ___
- [ ] **Tests failed:** ___
- [ ] **Tests skipped:** ___

### Analyze Failures

For each failure, record:

| Test Name | Error Message | Duration | Action Required |
|-----------|---------------|----------|-----------------|
| ___ | ___ | ___s | ___ |
| ___ | ___ | ___s | ___ |
| ___ | ___ | ___s | ___ |

### Performance Metrics

Record key metrics:

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| 30-day HDF upload | < 60s | ___s | ✓ / ✗ |
| PDF extraction | < 45s | ___s | ✓ / ✗ |
| Memory usage (peak) | < 2 GB | ___ MB | ✓ / ✗ |
| Memory growth (5 uploads) | < 10% | ___% | ✓ / ✗ |

### Clean Up

- [ ] **Stop any hanging processes:**
  ```bash
  pkill -f streamlit
  pkill -f pytest
  ```

- [ ] **Clean temp files:**
  ```bash
  rm -f /tmp/*.csv
  rm -f /tmp/pytest-*
  ```

- [ ] **Check port released:**
  ```bash
  lsof -i :8600  # Should return nothing
  ```

- [ ] **Review logs:** Check for warnings or errors

## Troubleshooting Guide

### Issue: Streamlit won't start

**Symptoms:** "Streamlit app did not start within 30 seconds"

**Solutions:**
1. Check Streamlit installed: `streamlit --version`
2. Try manual start: `streamlit run app/main.py --server.port 8600`
3. Check firewall settings
4. Verify Python version: `python3 --version` (should be 3.11+)

### Issue: Port already in use

**Symptoms:** "Address already in use: 8600"

**Solutions:**
```bash
# Find process
lsof -i :8600

# Kill process
kill -9 <PID>

# Or kill all Streamlit processes
pkill -f streamlit
```

### Issue: Tests hang indefinitely

**Symptoms:** Test runs for > 5 minutes without output

**Solutions:**
1. Press Ctrl+C to stop
2. Kill hung processes: `pkill -f pytest`
3. Increase timeout: `--timeout=300`
4. Check system resources (CPU, memory)

### Issue: Memory test fails

**Symptoms:** "Memory should not grow > 10%" fails

**Solutions:**
1. Close other applications
2. Monitor with: `top` or Activity Monitor
3. Check for leaks in app code
4. Verify temp files cleaned up: `ls -la /tmp/*.csv`

### Issue: Network simulation fails

**Symptoms:** Network tests timeout or fail

**Solutions:**
1. May be too slow for your system
2. Skip in CI: `pytest -m "performance and not network"`
3. Check route callbacks correct
4. Verify `page.unroute()` called

### Issue: Browser not installed

**Symptoms:** "Browser not found"

**Solutions:**
```bash
python3 -m playwright install chromium
python3 -m playwright install --with-deps chromium  # With system deps
```

## Success Criteria

Test run is considered successful if:

- [ ] ≥ 90% of tests pass (≥ 20/22)
- [ ] All critical tests pass:
  - [ ] Large file handling
  - [ ] Memory leak detection
  - [ ] Timeout handling
- [ ] Performance metrics within targets:
  - [ ] HDF upload < 60s
  - [ ] Memory growth < 10%
- [ ] No crashes or hangs
- [ ] Clear error messages on failures

## Reporting

### Generate Report

```bash
# HTML report
python3 -m pytest -m performance app/tests/test_e2e_performance.py -v --html=performance-report.html --self-contained-html

# JUnit XML (for CI)
python3 -m pytest -m performance app/tests/test_e2e_performance.py -v --junit-xml=performance-results.xml
```

### Report Locations

- HTML Report: `performance-report.html`
- XML Results: `performance-results.xml`
- Test output: Console / `test.log`

### Share Results

- [ ] Review HTML report in browser
- [ ] Share metrics with team
- [ ] Log failures in issue tracker
- [ ] Update performance baseline if needed

## Next Steps

After successful test run:

- [ ] Update performance benchmarks if metrics improved
- [ ] Fix any failed tests
- [ ] Add new tests for edge cases discovered
- [ ] Update documentation if behavior changed
- [ ] Set up CI/CD integration if not already done

## Sign-Off

**Executed by:** _______________
**Date:** _______________
**Environment:** macOS / Linux / CI
**Test duration:** ___ minutes
**Overall result:** PASS / FAIL / PARTIAL

**Notes:**
_____________________________________________
_____________________________________________
_____________________________________________

---

**Next test run due:** _______________
